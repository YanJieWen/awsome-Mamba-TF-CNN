name: vssm
model_type: VSSM
num_gpu: 1
manual_seed: 42

# dataset and data loader settings
datasets:
  name: cifar100
  data_type: CiFar100
  train:
    root: './data/datasets/cifar100'
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    gt_size: 224
    data_type: 'Train'

  val:
    root: './data/datasets/cifar100'
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    gt_size: 224
    data_type: 'Test'

#network structures
backbone:
  model_cfg:
    depths: [2, 2, 15, 2]
    dims: 96
    drop_path_rate: 0.3
    patch_size: 4
    in_chans: 3
    num_classes: 5
    ssm_d_state: 1
    ssm_ratio: 2.0
    ssm_dt_rank: 'auto'
    ssm_act_layer: "silu"
    ssm_conv: 3
    ssm_conv_bias: False
    ssm_drop_rate: 0.0
    ssm_init: "v0"
    forward_type: "v05_noz"
    mlp_ratio: 4.0
    mlp_act_layer: "gelu"
    mlp_drop_rate: 0.0
    gmlp: False
    patch_norm: True
    norm_layer: 'ln2d'
    downsample_version: "v3"
    patchembed_version: "v2"
    use_checkpoint: False
    posembed: False
    imgsize: 224
  ckpt_path: './models/pretrained/vmamba/vssm_small_0229_ckpt_epoch_222.pth'


# training settings
train:
  name: train
  engine_type: Trainer
  resume: " "
  optimizer:
    name: 'adamw' #adam,adamw,nadam,adamax,adadelta,adagrad,rmsprop,adabelif,nadamw,radam
    lr: 0.00005
    weight_decay: 0.005
    eps: 0.00000008
  lr_schedular:
    name: 'cosine' #cosine,multistep,poly,tanh
    t_initial: 3
    lr_min: 0.00001
    k_decay: 1.0
    cycle_mul: 1.
    cycle_decay: 1.
    cycle_limit: 1.
    warmup_t: 0.
    warmup_lr_init: 0.000001
    noise_pct: 0.67
    noise_std: 1.0
    noise_seed: 42
  criterion:
    name: 'smooth' #bce,smooth,ce
    reduction: 'mean'


# validation settings
val:
  name: 'eval'
  engine_type: 'Evaluator'
  metric:
    name: 'topk'

#save checkpoint
save:

